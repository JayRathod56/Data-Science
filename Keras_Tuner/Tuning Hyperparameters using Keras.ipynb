{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\jayan\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: ipython in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from keras-tuner) (8.3.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from keras-tuner) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from keras-tuner) (2.28.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from keras-tuner) (2.9.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (3.0.20)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.4.5)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.1.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (2.11.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (61.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.18.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from packaging->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (1.26.9)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from stack-data->ipython->keras-tuner) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from stack-data->ipython->keras-tuner) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from stack-data->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: six in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython->keras-tuner) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (3.20.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (2.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (2.6.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.42.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.8.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jayan\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\jayan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\jayan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\jayan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\jayan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\jayan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\jayan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.9.1\n",
      "KerasTuner Version: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"KerasTuner Version: {kt.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 6s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and split data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixels to values between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Performance\n",
    "\n",
    "As mentioned, we will first train a shallow dense neural network (DNN) with preselected\n",
    "hyperparameters giving us a baseline performance. We'll see later on how simple models, like this\n",
    "our shallow DNN, can take some time to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build baseline model with Sequential API\n",
    "b_model = keras.Sequential()\n",
    "b_model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "b_model.add(keras.layers.Dense(units=512, activation='relu', name='dense_1'))\n",
    "b_model.add(keras.layers.Dropout(0.2))\n",
    "b_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "# Print model summary\n",
    "b_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayan\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "b_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.5152 - accuracy: 0.8164 - val_loss: 0.4373 - val_accuracy: 0.8423\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.3916 - accuracy: 0.8576 - val_loss: 0.3816 - val_accuracy: 0.8594\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3550 - accuracy: 0.8697 - val_loss: 0.3474 - val_accuracy: 0.8767\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3347 - accuracy: 0.8765 - val_loss: 0.3319 - val_accuracy: 0.8827\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3176 - accuracy: 0.8812 - val_loss: 0.3364 - val_accuracy: 0.8831\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 10s 6ms/step - loss: 0.3029 - accuracy: 0.8885 - val_loss: 0.3221 - val_accuracy: 0.8812\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2934 - accuracy: 0.8906 - val_loss: 0.3172 - val_accuracy: 0.8848\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2820 - accuracy: 0.8942 - val_loss: 0.3197 - val_accuracy: 0.8842\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2753 - accuracy: 0.8962 - val_loss: 0.3101 - val_accuracy: 0.8901\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2672 - accuracy: 0.9003 - val_loss: 0.3327 - val_accuracy: 0.8786\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2571 - accuracy: 0.9021 - val_loss: 0.3130 - val_accuracy: 0.8907\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2495 - accuracy: 0.9054 - val_loss: 0.3081 - val_accuracy: 0.8919\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2428 - accuracy: 0.9081 - val_loss: 0.3186 - val_accuracy: 0.8891\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2377 - accuracy: 0.9096 - val_loss: 0.3172 - val_accuracy: 0.8887\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2327 - accuracy: 0.9126 - val_loss: 0.3071 - val_accuracy: 0.8934\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.2261 - accuracy: 0.9156 - val_loss: 0.3088 - val_accuracy: 0.8941\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2208 - accuracy: 0.9160 - val_loss: 0.3128 - val_accuracy: 0.8938\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2156 - accuracy: 0.9191 - val_loss: 0.3436 - val_accuracy: 0.8823\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2104 - accuracy: 0.9201 - val_loss: 0.3269 - val_accuracy: 0.8921\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.2045 - accuracy: 0.9216 - val_loss: 0.3265 - val_accuracy: 0.8935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b3cdeb3820>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of epochs\n",
    "NUM_EPOCHS = 20\n",
    "# Early stopping set after 5 epochs\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Train model\n",
    "b_model.fit(X_train, y_train, epochs=NUM_EPOCHS, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "b_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3643 - accuracy: 0.8849\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.364314</td>\n",
       "      <td>0.8849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss accuracy\n",
       "Baseline  0.364314   0.8849"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    eval_dict = model.evaluate(X_test, y_test, return_dict=True)\n",
    "    display_df = pd.DataFrame([eval_dict.values()], columns=[list(eval_dict.keys())])\n",
    "    return display_df\n",
    "\n",
    "results = evaluate_model(b_model, X_test, y_test)\n",
    "results.index = ['Baseline']\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's the results for a single set of hyperparameters. Imagine trying out different learning rates,\n",
    "dropout percentages, number of hidden layers, and number of neurons in each hidden layer. As you\n",
    "can see, manual hypertuning is simply not feasible nor scalable. In the next section you'll see how\n",
    "Keras Tuner solves these problems simply by automating the process and searching the\n",
    "hyperparameter space in an efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner\n",
    "Keras Tuner is a simple, distributable hyperparameter optimization framework that automates the\n",
    "painful process of manually searching for optimal hyperparameters. Keras Tuner comes with\n",
    "Random Search, Hyperband, and Bayesian Optimization built-in search algorithms, and is designed\n",
    "to fit many use cases including:\n",
    "* Distributed tuning\n",
    "* Custom training loops (e.g., GANs, reinforcement learning, etc.)\n",
    "* Adding hyperparameters outside of the model building function (preprocessing, data\n",
    "augmentation, test time augmentation, etc.)\n",
    "\n",
    "These processes are outside the scope of this write-up, but feel free to read more in the official\n",
    "documentation. There are four steps to hypertune our shallow DNN using Keras Tuner:\n",
    "1. Define the model\n",
    "2. Specify which hyperparameters to tune\n",
    "3. Define the search space\n",
    "4. Define the search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "\n",
    "    # Initialize sequential API and start building model.\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "    # Tune the number of hidden layers and units in each.\n",
    "    # Number of hidden layers: 1 - 5\n",
    "    # Number of Units: 32 - 512 with stepsize of 32\n",
    "    for i in range(1, hp.Int(\"num_layers\", 2, 6)):\n",
    "        model.add(\n",
    "        keras.layers.Dense(\n",
    "        units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "        activation=\"relu\")\n",
    "    )\n",
    "        # Tune dropout layer with values from 0 - 0.3.\n",
    "        model.add(keras.layers.Dropout(hp.Float(\"dropout_\" + str(i), 0, 0.3, step=0.1)))\n",
    "    # Add output layer.\n",
    "    model.add(keras.layers.Dense(units=10, activation=\"softmax\"))\n",
    "    # Tune learning rate for Adam optimizer with values from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(build_model,\n",
    "objective=\"val_accuracy\",\n",
    "max_epochs=20,\n",
    "factor=3,\n",
    "hyperband_iterations=1,\n",
    "directory=\"kt_dir\",\n",
    "project_name=\"kt_hyperband\",\n",
    "overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 6, 'step': 1, 'sampling': None}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "dropout_1 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': 0.1, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "# Display search space summary\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 48s]\n",
      "val_accuracy: 0.8523333072662354\n",
      "\n",
      "Best val_accuracy So Far: 0.8952500224113464\n",
      "Total elapsed time: 00h 26m 27s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# This cell takes a long time to run when hyperband_iterations is large\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(X_train, y_train, epochs=NUM_EPOCHS, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 480)               376800    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 160)               76960     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 448)               72128     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 448)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                4490      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 530,378\n",
      "Trainable params: 530,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters from the results\n",
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "h_model = tuner.hypermodel.build(best_hps)\n",
    "h_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.6272 - accuracy: 0.7837 - val_loss: 0.4381 - val_accuracy: 0.8461\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.4206 - accuracy: 0.8503 - val_loss: 0.3832 - val_accuracy: 0.8621\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3740 - accuracy: 0.8658 - val_loss: 0.3636 - val_accuracy: 0.8704\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3444 - accuracy: 0.8743 - val_loss: 0.3417 - val_accuracy: 0.8749\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3219 - accuracy: 0.8819 - val_loss: 0.3395 - val_accuracy: 0.8754\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3059 - accuracy: 0.8881 - val_loss: 0.3337 - val_accuracy: 0.8781\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2920 - accuracy: 0.8938 - val_loss: 0.3195 - val_accuracy: 0.8853\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.2797 - accuracy: 0.8974 - val_loss: 0.3108 - val_accuracy: 0.8868\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2687 - accuracy: 0.9010 - val_loss: 0.3101 - val_accuracy: 0.8883\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2610 - accuracy: 0.9035 - val_loss: 0.3170 - val_accuracy: 0.8854\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.2507 - accuracy: 0.9080 - val_loss: 0.2960 - val_accuracy: 0.8923\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2409 - accuracy: 0.9103 - val_loss: 0.3127 - val_accuracy: 0.8855\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2340 - accuracy: 0.9124 - val_loss: 0.3356 - val_accuracy: 0.8783\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2255 - accuracy: 0.9172 - val_loss: 0.3186 - val_accuracy: 0.8891\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2185 - accuracy: 0.9200 - val_loss: 0.3048 - val_accuracy: 0.8938\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2125 - accuracy: 0.9198 - val_loss: 0.3042 - val_accuracy: 0.8914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b3d44b5490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the hypertuned model\n",
    "h_model.fit(X_train, y_train, epochs=NUM_EPOCHS, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayan\\AppData\\Local\\Temp\\ipykernel_16152\\3116713322.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results.append(hyper_df)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.364314</td>\n",
       "      <td>0.8849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypertuned</th>\n",
       "      <td>0.330538</td>\n",
       "      <td>0.8859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                loss accuracy\n",
       "Baseline    0.364314   0.8849\n",
       "Hypertuned  0.330538   0.8859"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_df = evaluate_model(h_model, X_test, y_test)\n",
    "hyper_df.index = [\"Hypertuned\"]\n",
    "results.append(hyper_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d2dbc107e90016ff58d506d19dfa66132cafa2ebfedbf9f4d98cf0f5094b18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
