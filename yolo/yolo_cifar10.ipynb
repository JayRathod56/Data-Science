{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D,Activation, SpatialDropout2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, SeparableConv2D\n",
    "from keras.layers import Concatenate\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdgen = ImageDataGenerator(\n",
    "    featurewise_center = False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center = False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization = False,  # divide each input by its std\n",
    "    zca_whitening = False,  # apply ZCA whitening\n",
    "    rotation_range = 15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range = 0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range = 0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip = True,  # randomly flip images\n",
    "    vertical_flip = False,  # randomly flip images\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    ")\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "imdgen.fit(x_train)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "dgen = imdgen.flow(x_train, y_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 32, 32, 16)        432       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 32, 32, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 16, 16, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 32)        4608      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 16, 16, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 64)          18432     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 128)         73728     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 2, 2, 512)         589824    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 2, 2, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 710,522\n",
      "Trainable params: 709,018\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(32,32,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "for i in range(0,3):\n",
    "    model.add(Conv2D(32*(2**i), (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint(\"CIFAR_10_YOLO_with_dropout.h5\", monitor='val_loss', verbose=2, save_best_only=True, mode='min',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8703 - categorical_accuracy: 0.3387\n",
      "Epoch 1: val_loss improved from inf to 1.80622, saving model to CIFAR_10_YOLO_with_dropout.h5\n",
      "391/391 [==============================] - 100s 253ms/step - loss: 1.8703 - categorical_accuracy: 0.3387 - val_loss: 1.8062 - val_categorical_accuracy: 0.3648\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5411 - categorical_accuracy: 0.4400\n",
      "Epoch 2: val_loss did not improve from 1.80622\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 1.5411 - categorical_accuracy: 0.4400 - val_loss: 1.8808 - val_categorical_accuracy: 0.3744\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4114 - categorical_accuracy: 0.4901\n",
      "Epoch 3: val_loss improved from 1.80622 to 1.56503, saving model to CIFAR_10_YOLO_with_dropout.h5\n",
      "391/391 [==============================] - 81s 208ms/step - loss: 1.4114 - categorical_accuracy: 0.4901 - val_loss: 1.5650 - val_categorical_accuracy: 0.4533\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3078 - categorical_accuracy: 0.5259\n",
      "Epoch 4: val_loss improved from 1.56503 to 1.35021, saving model to CIFAR_10_YOLO_with_dropout.h5\n",
      "391/391 [==============================] - 78s 199ms/step - loss: 1.3078 - categorical_accuracy: 0.5259 - val_loss: 1.3502 - val_categorical_accuracy: 0.5111\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2347 - categorical_accuracy: 0.5559\n",
      "Epoch 5: val_loss improved from 1.35021 to 1.20478, saving model to CIFAR_10_YOLO_with_dropout.h5\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 1.2347 - categorical_accuracy: 0.5559 - val_loss: 1.2048 - val_categorical_accuracy: 0.5618\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1731 - categorical_accuracy: 0.5803\n",
      "Epoch 6: val_loss did not improve from 1.20478\n",
      "391/391 [==============================] - 80s 204ms/step - loss: 1.1731 - categorical_accuracy: 0.5803 - val_loss: 1.3384 - val_categorical_accuracy: 0.5314\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1257 - categorical_accuracy: 0.5953\n",
      "Epoch 7: val_loss did not improve from 1.20478\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 1.1257 - categorical_accuracy: 0.5953 - val_loss: 1.2544 - val_categorical_accuracy: 0.5656\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0863 - categorical_accuracy: 0.6117\n",
      "Epoch 8: val_loss did not improve from 1.20478\n",
      "391/391 [==============================] - 80s 206ms/step - loss: 1.0863 - categorical_accuracy: 0.6117 - val_loss: 1.2612 - val_categorical_accuracy: 0.5560\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0467 - categorical_accuracy: 0.6237\n",
      "Epoch 9: val_loss improved from 1.20478 to 1.12657, saving model to CIFAR_10_YOLO_with_dropout.h5\n",
      "391/391 [==============================] - 82s 211ms/step - loss: 1.0467 - categorical_accuracy: 0.6237 - val_loss: 1.1266 - val_categorical_accuracy: 0.5932\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0166 - categorical_accuracy: 0.6390\n",
      "Epoch 10: val_loss improved from 1.12657 to 1.03540, saving model to CIFAR_10_YOLO_with_dropout.h5\n",
      "391/391 [==============================] - 80s 205ms/step - loss: 1.0166 - categorical_accuracy: 0.6390 - val_loss: 1.0354 - val_categorical_accuracy: 0.6339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f106857280>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train,batch_size=128,epochs=10,validation_data=(x_test, y_test),callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = model.evaluate(x_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 98.076 % , Test loss : 103.540 %\n",
      "Train accuracy : 64.742 % , Test accuracy : 63.390 %\n"
     ]
    }
   ],
   "source": [
    "print('Train loss : %.3f %% , Test loss : %.3f %%' % (train_mse[0]*100, test_mse[0]*100))\n",
    "print('Train accuracy : %.3f %% , Test accuracy : %.3f %%' % (train_mse[1]*100, test_mse[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d2dbc107e90016ff58d506d19dfa66132cafa2ebfedbf9f4d98cf0f5094b18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
