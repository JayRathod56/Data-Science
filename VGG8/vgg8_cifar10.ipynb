{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NOFnpLj6zIBh"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, Reshape, Activation\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.metrics import log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7kYtzTS50Zqp"
      },
      "outputs": [],
      "source": [
        "def vgg8_model(img_rows, img_cols, channel):\n",
        "   \n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D((1, 1), input_shape=(img_rows, img_cols, channel)))\n",
        "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D(strides=(2,2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D(strides=(2,2)))\n",
        "\n",
        "    # Add Fully Connected Layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(3072, activation='relu'))\n",
        "    model.add(Dense(2048, activation='relu'))\n",
        "    model.add(Dense(2048, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Learning rate is changed to 0.001\n",
        "    sgd = SGD(learning_rate=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # adm = Adam(learning_rate=0.001) \n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cUHZL6c-VuTZ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "\n",
        "nb_train_samples = 10000 # 5000 training samples\n",
        "nb_valid_samples = 1000 # 300 validation samples\n",
        "num_classes = 10\n",
        "\n",
        "def load_cifar10_data(img_rows, img_cols):\n",
        "\n",
        "    # Load cifar10 training and validation sets\n",
        "    (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n",
        "\n",
        "    # Resize trainging images\n",
        "    if K.image_data_format() == 'th':\n",
        "        X_train = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_train[:nb_train_samples,:,:,:]])\n",
        "        X_valid = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
        "    else:\n",
        "        X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:nb_train_samples,:,:,:]])\n",
        "        X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
        "\n",
        "    # Transform targets to keras compatible format\n",
        "    Y_train = np_utils.to_categorical(Y_train[:nb_train_samples], num_classes)\n",
        "    Y_valid = np_utils.to_categorical(Y_valid[:nb_valid_samples], num_classes)\n",
        "\n",
        "    return X_train, Y_train, X_valid, Y_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBhB2xpP1Qpa",
        "outputId": "3f829d94-a056-465e-cc3a-e2f234405bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Epoch 1/30\n",
            "100/100 - 65s - loss: 2.2693 - accuracy: 0.1343 - val_loss: 2.1064 - val_accuracy: 0.2310 - 65s/epoch - 653ms/step\n",
            "Epoch 2/30\n",
            "100/100 - 56s - loss: 1.9541 - accuracy: 0.2872 - val_loss: 1.8239 - val_accuracy: 0.3530 - 56s/epoch - 563ms/step\n",
            "Epoch 3/30\n",
            "100/100 - 54s - loss: 1.7534 - accuracy: 0.3715 - val_loss: 1.7066 - val_accuracy: 0.3760 - 54s/epoch - 543ms/step\n",
            "Epoch 4/30\n",
            "100/100 - 60s - loss: 1.6393 - accuracy: 0.4112 - val_loss: 1.6462 - val_accuracy: 0.4090 - 60s/epoch - 595ms/step\n",
            "Epoch 5/30\n",
            "100/100 - 54s - loss: 1.5508 - accuracy: 0.4485 - val_loss: 1.6230 - val_accuracy: 0.4080 - 54s/epoch - 539ms/step\n",
            "Epoch 6/30\n",
            "100/100 - 61s - loss: 1.4860 - accuracy: 0.4678 - val_loss: 1.5780 - val_accuracy: 0.4380 - 61s/epoch - 609ms/step\n",
            "Epoch 7/30\n",
            "100/100 - 59s - loss: 1.4226 - accuracy: 0.4942 - val_loss: 1.5147 - val_accuracy: 0.4650 - 59s/epoch - 587ms/step\n",
            "Epoch 8/30\n",
            "100/100 - 55s - loss: 1.3441 - accuracy: 0.5183 - val_loss: 1.5321 - val_accuracy: 0.4360 - 55s/epoch - 551ms/step\n",
            "Epoch 9/30\n",
            "100/100 - 58s - loss: 1.2964 - accuracy: 0.5430 - val_loss: 1.4805 - val_accuracy: 0.4650 - 58s/epoch - 581ms/step\n",
            "Epoch 10/30\n",
            "100/100 - 58s - loss: 1.2145 - accuracy: 0.5712 - val_loss: 1.4915 - val_accuracy: 0.4740 - 58s/epoch - 581ms/step\n",
            "Epoch 11/30\n",
            "100/100 - 58s - loss: 1.1718 - accuracy: 0.5856 - val_loss: 1.5166 - val_accuracy: 0.4810 - 58s/epoch - 580ms/step\n",
            "Epoch 12/30\n",
            "100/100 - 57s - loss: 1.0952 - accuracy: 0.6152 - val_loss: 1.5005 - val_accuracy: 0.4940 - 57s/epoch - 574ms/step\n",
            "Epoch 13/30\n",
            "100/100 - 54s - loss: 1.0372 - accuracy: 0.6330 - val_loss: 1.5656 - val_accuracy: 0.4620 - 54s/epoch - 538ms/step\n",
            "Epoch 14/30\n",
            "100/100 - 54s - loss: 0.9738 - accuracy: 0.6603 - val_loss: 1.5299 - val_accuracy: 0.4920 - 54s/epoch - 542ms/step\n",
            "Epoch 15/30\n",
            "100/100 - 57s - loss: 0.8966 - accuracy: 0.6893 - val_loss: 1.5820 - val_accuracy: 0.4690 - 57s/epoch - 570ms/step\n",
            "Epoch 16/30\n",
            "100/100 - 57s - loss: 0.8223 - accuracy: 0.7102 - val_loss: 1.6345 - val_accuracy: 0.4800 - 57s/epoch - 568ms/step\n",
            "Epoch 17/30\n",
            "100/100 - 55s - loss: 0.7493 - accuracy: 0.7360 - val_loss: 1.8061 - val_accuracy: 0.4580 - 55s/epoch - 554ms/step\n",
            "Epoch 18/30\n",
            "100/100 - 55s - loss: 0.6718 - accuracy: 0.7655 - val_loss: 1.8023 - val_accuracy: 0.4710 - 55s/epoch - 548ms/step\n",
            "Epoch 19/30\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # Example to fine-tune on 5000 samples from Cifar10\n",
        "\n",
        "    img_rows, img_cols = 224, 224 # Resolution of inputs\n",
        "    channel = 3 \n",
        "    batch_size = 100 \n",
        "    nb_epoch = 30\n",
        "\n",
        "    \n",
        "\n",
        "    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\n",
        "    X_train, Y_train, X_valid, Y_valid = load_cifar10_data(img_rows, img_cols)\n",
        "   \n",
        "    \n",
        "\n",
        "    # Load vgg7 our model\n",
        "    model = vgg8_model(img_rows, img_cols, channel)\n",
        "\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Runnin on GPU\n",
        "    with tf.device('/gpu:0'):\n",
        "        # Start Fine-tuning\n",
        "        print('Using GPU')\n",
        "        model.fit(X_train, Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=nb_epoch,\n",
        "              shuffle=True,\n",
        "              verbose=1,\n",
        "              validation_data=(X_valid, Y_valid),\n",
        "              )\n",
        "\n",
        "    # Make predictions\n",
        "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    # Cross-entropy loss score\n",
        "    score = log_loss(Y_valid, predictions_valid)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "vgg16_fineTune_Student_Copy.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "51d2dbc107e90016ff58d506d19dfa66132cafa2ebfedbf9f4d98cf0f5094b18"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
