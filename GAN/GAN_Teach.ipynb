{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0jjwNClOjqw",
        "outputId": "226fc4ba-d679-40d0-9980-23026e57ebee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported\n"
          ]
        }
      ],
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, ReLU, LeakyReLU, Dense\n",
        "from keras.layers.core import Activation, Reshape\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.core import Flatten\n",
        "#from keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from keras.datasets import cifar10\n",
        "from keras import initializers\n",
        "\n",
        "print(\"Libraries imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "77SG83oaPSwI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cifar10 data loaded\n"
          ]
        }
      ],
      "source": [
        "def get_data():\n",
        "    # load cifar10 data\n",
        "    (X_train, _), (X_test, _) = cifar10.load_data()\n",
        "\n",
        "    # convert train and test data to float32\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # scale train and test data to [0, 1]\n",
        "    X_train = (X_train / 255) \n",
        "    X_test = (X_train / 255) \n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "print(\"cifar10 data loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "# x = X_train\n",
        "# check = np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
        "# print(check.max())\n",
        "# print(check.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GDHfLmILPH0H"
      },
      "outputs": [],
      "source": [
        "def build_cifar10_discriminator(ndf=64, image_shape=(32, 32, 3)):\n",
        "    \"\"\" Builds CIFAR10 DCGAN Discriminator Model\n",
        "    PARAMS\n",
        "    ------\n",
        "    ndf: number of discriminator filters\n",
        "    image_shape: 32x32x3\n",
        "\n",
        "    RETURN\n",
        "    ------\n",
        "    D: keras sequential\n",
        "    \"\"\"\n",
        "    init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "    D = Sequential()\n",
        "\n",
        "    # Conv 1: 16x16x64\n",
        "    D.add(Conv2D(ndf, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init,\n",
        "                 input_shape=image_shape))\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 2: 8x8x128\n",
        "    D.add(Conv2D(ndf*2, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 3: 4x4x256\n",
        "    D.add(Conv2D(ndf*4, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 4:  2x2x512\n",
        "    D.add(Conv2D(ndf*8, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Flatten: 2x2x512 -> (2048)\n",
        "    D.add(Flatten())\n",
        "\n",
        "    # Dense Layer\n",
        "    D.add(Dense(1, kernel_initializer=init))\n",
        "    D.add(Activation('sigmoid'))\n",
        "\n",
        "    print(\"\\nDiscriminator\")\n",
        "    D.summary()\n",
        "\n",
        "    return D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zMhKbx-fPPgq"
      },
      "outputs": [],
      "source": [
        "def build_cifar10_generator(ngf=64, z_dim=128):\n",
        "    \"\"\" Builds CIFAR10 DCGAN Generator Model\n",
        "    PARAMS\n",
        "    ------\n",
        "    ngf: number of generator filters\n",
        "    z_dim: number of dimensions in latent vector\n",
        "\n",
        "    RETURN\n",
        "    ------\n",
        "    G: keras sequential\n",
        "    \"\"\"\n",
        "    init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "    G = Sequential()\n",
        "\n",
        "    # Dense 1: 2x2x512\n",
        "    G.add(Dense(2*2*ngf*8, input_shape=(z_dim, ),\n",
        "        use_bias=True, kernel_initializer=init))\n",
        "    G.add(Reshape((2, 2, ngf*8)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 1: 4x4x256\n",
        "    G.add(Conv2DTranspose(ngf*4, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 2: 8x8x128\n",
        "    G.add(Conv2DTranspose(ngf*2, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 3: 16x16x64\n",
        "    G.add(Conv2DTranspose(ngf, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 4: 32x32x3\n",
        "    G.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(Activation('tanh'))\n",
        "\n",
        "    print(\"\\nGenerator\")\n",
        "    G.summary()\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NDZvShxbPidh"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, filename):\n",
        "    h, w, c = images.shape[1:]\n",
        "    grid_size = ceil(np.sqrt(images.shape[0]))\n",
        "    images = (images.reshape(grid_size, grid_size, h, w, c)\n",
        "              .transpose(0, 2, 1, 3, 4)\n",
        "              .reshape(grid_size*h, grid_size*w, c))\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.imsave(filename, images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VWMSUdJ-Pj1D"
      },
      "outputs": [],
      "source": [
        "def plot_losses(losses_d, losses_g, filename):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "    axes[0].plot(losses_d)\n",
        "    axes[1].plot(losses_g)\n",
        "    axes[0].set_title(\"losses_d\")\n",
        "    axes[1].set_title(\"losses_g\")\n",
        "    plt.tight_layout()#\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8efaMdhWVlwu"
      },
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize a list of sample image data in the range of 0 to 1\n",
        "    : x: List of image data.  The image shape is (32, 32, 3)\n",
        "    : return: Numpy array of normalized data\n",
        "    \"\"\"\n",
        "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4xwPo4fPl3_",
        "outputId": "b5201692-5fe9-4eda-b6c7-657c51ca1e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image shape (32, 32, 3), min val 0.0, max val 1.0\n",
            "\n",
            "Discriminator\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 64)        4864      \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         204928    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 256)         819456    \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_10 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 2, 2, 512)         3277312   \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 2, 2, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_11 (LeakyReLU)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 2049      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,312,193\n",
            "Trainable params: 4,310,401\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "\n",
            "Generator\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 2048)              206848    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 2, 2, 512)        2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_12 (LeakyReLU)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 4, 4, 256)        3277056   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 4, 4, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_13 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2DT  (None, 8, 8, 128)        819328    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_14 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_6 (Conv2DT  (None, 16, 16, 64)       204864    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_7 (Conv2DT  (None, 32, 32, 3)        4803      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,516,739\n",
            "Trainable params: 4,514,819\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "Epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jayan\\AppData\\Local\\Temp\\ipykernel_19928\\2196653866.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss_d=0.00002, loss_g=0.00001\n",
            "2/2 [==============================] - 0s 28ms/step\n",
            "\tPlotting images and losses\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Floating point image RGB values must be in the 0..1 range.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32me:\\Jay\\Data Science Sem 3\\GAN\\GAN_Teach.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000007?line=79'>80</a>\u001b[0m \u001b[39m# Runnin on GPU\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000007?line=80'>81</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/gpu:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000007?line=81'>82</a>\u001b[0m     train()\n",
            "\u001b[1;32me:\\Jay\\Data Science Sem 3\\GAN\\GAN_Teach.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(ndf, ngf, z_dim, lr_d, lr_g, epochs, batch_size, epoch_per_checkpoint, n_checkpoint_images)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000007?line=72'>73</a>\u001b[0m fake_images \u001b[39m=\u001b[39m G\u001b[39m.\u001b[39mpredict(z_fixed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000007?line=73'>74</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mPlotting images and losses\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000007?line=74'>75</a>\u001b[0m plot_images(fake_images, \u001b[39m\"\u001b[39;49m\u001b[39mfake_images_e\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(e))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000007?line=75'>76</a>\u001b[0m plot_losses(losses_d, losses_g, \u001b[39m\"\u001b[39m\u001b[39mlosses.png\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;32me:\\Jay\\Data Science Sem 3\\GAN\\GAN_Teach.ipynb Cell 5'\u001b[0m in \u001b[0;36mplot_images\u001b[1;34m(images, filename)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000004?line=3'>4</a>\u001b[0m images \u001b[39m=\u001b[39m (images\u001b[39m.\u001b[39mreshape(grid_size, grid_size, h, w, c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000004?line=4'>5</a>\u001b[0m           \u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000004?line=5'>6</a>\u001b[0m           \u001b[39m.\u001b[39mreshape(grid_size\u001b[39m*\u001b[39mh, grid_size\u001b[39m*\u001b[39mw, c))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000004?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m16\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Jay/Data%20Science%20Sem%203/GAN/GAN_Teach.ipynb#ch0000004?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39;49mimsave(filename, images)\n",
            "File \u001b[1;32mc:\\Users\\jayan\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:2144\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, **kwargs)\u001b[0m\n\u001b[0;32m   2142\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave)\n\u001b[0;32m   2143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimsave\u001b[39m(fname, arr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2144\u001b[0m     \u001b[39mreturn\u001b[39;00m matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave(fname, arr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\jayan\\anaconda3\\lib\\site-packages\\matplotlib\\image.py:1641\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     rgba \u001b[39m=\u001b[39m arr\n\u001b[0;32m   1640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1641\u001b[0m     rgba \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mto_rgba(arr, \u001b[39mbytes\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1642\u001b[0m \u001b[39mif\u001b[39;00m pil_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1643\u001b[0m     pil_kwargs \u001b[39m=\u001b[39m {}\n",
            "File \u001b[1;32mc:\\Users\\jayan\\anaconda3\\lib\\site-packages\\matplotlib\\cm.py:437\u001b[0m, in \u001b[0;36mScalarMappable.to_rgba\u001b[1;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m xx\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m norm \u001b[39mand\u001b[39;00m (xx\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m xx\u001b[39m.\u001b[39mmin() \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> 437\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFloating point image RGB values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mmust be in the 0..1 range.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mbytes\u001b[39m:\n\u001b[0;32m    440\u001b[0m         xx \u001b[39m=\u001b[39m (xx \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n",
            "\u001b[1;31mValueError\u001b[0m: Floating point image RGB values must be in the 0..1 range."
          ]
        }
      ],
      "source": [
        "def train(ndf=64, ngf=64, z_dim=100, lr_d=2e-4, lr_g=2e-4, epochs=100,\n",
        "          batch_size=128, epoch_per_checkpoint=1, n_checkpoint_images=36):\n",
        "\n",
        "    X_train, _ = get_data()\n",
        "    image_shape = X_train[0].shape\n",
        "    print(\"image shape {}, min val {}, max val {}\".format(\n",
        "        image_shape, X_train[0].min(), X_train[0].max()))\n",
        "    X_train = normalize(X_train)\n",
        "\n",
        "    # plot real images for reference\n",
        "    plot_images(X_train[:n_checkpoint_images], \"real_images.png\")\n",
        "    \n",
        "\n",
        "    # build models\n",
        "    D = build_cifar10_discriminator(ndf, image_shape)\n",
        "    G = build_cifar10_generator(ngf, z_dim)\n",
        "\n",
        "    # define Discriminator's optimizer\n",
        "    D.compile(Adam(learning_rate=lr_d, beta_1=0.5), loss='binary_crossentropy',\n",
        "              metrics=['binary_accuracy'])\n",
        "\n",
        "    # define D(G(z)) graph for training the Generator\n",
        "    D.trainable = False\n",
        "    z = Input(shape=(z_dim, ))\n",
        "    D_of_G = Model(inputs=z, outputs=D(G(z)))\n",
        "\n",
        "    # define Generator's Optimizer\n",
        "    D_of_G.compile(Adam(learning_rate=lr_g, beta_1=0.5), loss='binary_crossentropy',\n",
        "                   metrics=['binary_accuracy'])\n",
        "\n",
        "    # get labels for computing the losses\n",
        "    labels_real = np.ones(shape=(batch_size, 1))\n",
        "    labels_fake = np.zeros(shape=(batch_size, 1))\n",
        "\n",
        "    losses_d, losses_g = [], []\n",
        "\n",
        "    # fix a z vector for training evaluation\n",
        "    z_fixed = np.random.uniform(0, 1, size=(n_checkpoint_images, z_dim))\n",
        "\n",
        "    # training loop\n",
        "    for e in range(epochs):\n",
        "        print(\"Epoch {}\".format(e))\n",
        "        for i in range(len(X_train) // batch_size):\n",
        "\n",
        "            # update Discriminator weights\n",
        "            D.trainable = True\n",
        "\n",
        "            # Get real samples\n",
        "            real_images = X_train[i*batch_size:(i+1)*batch_size]\n",
        "            loss_d_real = D.train_on_batch(x=real_images, y=labels_real)[0]\n",
        "\n",
        "            # Fake Samples\n",
        "            z = np.random.uniform(0, 1, size=(batch_size, z_dim))\n",
        "            fake_images = G.predict_on_batch(z)\n",
        "            loss_d_fake = D.train_on_batch(x=fake_images, y=labels_fake)[0]\n",
        "\n",
        "            # Compute Discriminator's loss\n",
        "            loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
        "\n",
        "            # update Generator weights, do not update Discriminator weights\n",
        "            D.trainable = False\n",
        "            loss_g = D_of_G.train_on_batch(x=z, y=labels_real)[0]\n",
        "\n",
        "        losses_d.append(loss_d)\n",
        "        \n",
        "        losses_g.append(loss_g)\n",
        "        losses_d = normalize(losses_d)\n",
        "        losses_g = normalize(losses_g)\n",
        "\n",
        "\n",
        "        if (e % epoch_per_checkpoint) == 0:\n",
        "            print(\"loss_d={:.5f}, loss_g={:.5f}\".format(loss_d, loss_g))\n",
        "            fake_images = G.predict(z_fixed)\n",
        "            print(\"\\tPlotting images and losses\")\n",
        "            plot_images(fake_images, \"fake_images_e{}.png\".format(e))\n",
        "            plot_losses(losses_d, losses_g, \"losses.png\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Runnin on GPU\n",
        "with tf.device('/gpu:0'):\n",
        "    train()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "GAN_Teach.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "51d2dbc107e90016ff58d506d19dfa66132cafa2ebfedbf9f4d98cf0f5094b18"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
